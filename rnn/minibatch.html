<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118361649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118361649-1');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
<title>Training an RNN with vectorized minibatch SGD</title>
<!-- META -->
<!-- LinkedIn meta -->
<meta property='og:title' content="Explaining RNNs without neural networks"/>
<meta property='og:image' content="http://explained.ai/rnn/images/">
<meta property='og:description' content=""/>
<meta property='og:url' content="http://explained.ai/rnn/index.html"/>

<!-- Facebook meta -->
<meta property="og:type" content="article" />

<!-- Twitter meta -->
<meta name="twitter:title" content="Explaining RNNs without neural networks">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@the_antlr_guy">
<meta name="twitter:creator" content="@the_antlr_guy">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="http://explained.ai/rnn/images/">
<!-- END META -->
</head>
<body>
<div class="watermark">
<i><a href='http://explained.ai/rnn/index.html'>Main article</a><br>Brought to you by <a href='http://explained.ai'>explained.ai</a></i><br>
</div>

<h1>2 Training an RNN with vectorized minibatch SGD</h1>

<p></p>

<p><a href="http://parrt.cs.usfca.edu">Terence Parr</a></p>

<p style="font-size: 80%">(Terence teaches in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a>. You might know Terence as the creator of the ANTLR parser generator.)</p>



<div id="toc">
<p class="toc_title">Contents</p>
<ul>
	<li><a href="#sec:2.1">Computing loss using more than one input record</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:2.2">Converting the batch loop to vector operations</a>
	<ul>
			<li><a href="#sec:2.2.1">Processing batches, one character position at a time</a></li>
			<li><a href="#sec:2.2.2">Padding short words with 0 vectors on the left</a></li>

	</ul>
	</li>

</ul>
</div>


<p>	 In the previous section, we trained an RNN by updating model parameters <span class=eqn>W</span>, <span class=eqn>U</span>, <span class=eqn>V</span> after each input record (each word). The training loop computed the loss for a single record and used that to estimate partial derivatives. Those &ldquo;noisy&rdquo; partial derivatives (computed by backpropagation) were then used by the gradient descent optimizer to tweak the matrices. The goal of this section is to make a more efficient version of the training loop by (1) performing backpropagation less frequently and (2) using PyTorch vector operations (that are implemented in C++) to avoid Python a <span class=inlinecode>for</span>-loop.  </p>

<p>The idea of breaking up a training set into chunks is an important implementation detail, but keep in mind that it really has nothing to do with the concept of an RNN.  The words in our training set are very short so we can focus on slicing our training set in between input records.   If our input records were long documents instead of just simple words, we'd likely need to slice up even the individual records, not just between records. Long documents, as we'd use for a language model, complicate our lives and I'll leave that for another article. Or, you can check out section &ldquo;Maintaining the State of an RNN&rdquo; in fastai Chapter 12 or &ldquo;The need for truncated backpropagation&rdquo; in Trask Chapter 14.</p>



<h2 id="sec:2.1">2.1 Computing loss using more than one input record</h2>


<p>If computing partial derivatives is expensive, we want to avoid doing it after computing the <span class=inlinecode>h</span> vector for each word. The subset of input records we use to compute the loss is called a <i>minibatch</i>.  The size of the minibatch is a knob we can turn: the larger the batches, the more accurate our partial derivative estimates, but our validation loss is likely to be higher. Here's a fun tweet and link to supporting evidence from LeCun:</p>

<p>	<center>
<a href="https://twitter.com/ylecun/status/989610208497360896">
<img src="images/lecunn-batch-size.png" width="30%" url="https://twitter.com/ylecun/status/989610208497360896">
</a>
</center>
</p>

<p>To see how minibatching works, let's add <i>kat</i>, the Afrikaans word, to our classification problem so we have two batches of size two (the character vocabulary is the same):</p>

<p><center>
<a href="images/word-to-num-4.svg">
<img src="images/word-to-num-4.svg" width="25%" url="images/word-to-num-4.svg">
</a>
</center>
</p>

<p>	To process records in batches instead of one at a time, we need a triply-nested loop instead of a doubly-nested loop. Previously we had an outer loop that iterated through each training record and an inner loop that processed the characters of a word. We still have the same inner loop, but we need to split the outer loop into one that advances from batch to batch and a nested loop that iterates through each batch:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="44%" align="center">SGD training loop</th><th valign=top align="center">Non-vectorized minibatch SGD training loop</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">

<center>
<center>
<a href="images/batch-side-by-side-A.png">
<img src="images/batch-side-by-side-A.png" width="100%" url="images/batch-side-by-side-A.png">
</a>
</center>

</center>

</td><td valign=top align="center">

<center>
<center>
<a href="images/batch-side-by-side-B.png">
<img src="images/batch-side-by-side-B.png" width="100%" url="images/batch-side-by-side-B.png">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>(Of course, the real implementation would require yet another outer loop to iterate over multiple epochs.)</p>

<p>I've highlighted the inner loop, which is identical for both cases and I have circled the key difference in the loss computation. Instead of computing a single loss for a single <span class=inlinecode>h</span> vector, we are summing up the losses for a batch of <span class=inlinecode>h</span> vectors.  </p>

<p>The effect of minibatching is to delay updating the matrices representing the parameters of our model until after we've seen more than a single input record. This gives us a more accurate gradient and is a little more efficient because were not computing gradients for every input record. Because the gradient is more accurate, we can often afford (and often need) a higher learning rate than the one we use for single record gradient descent. You can check out the full implementation in the <a href="https://github.com/parrt/ml-articles/blob/master/rnn/notebooks/minibatch.ipynb">minibatch SGD notebook</a>.  </p>

<p>Reducing how often we compute partial derivatives increases training speed, but the biggest speed bump comes from converting the loop that iterates through a batch into vector operations.</p>



<h2 id="sec:2.2">2.2 Converting the batch loop to vector operations</h2>


<p>In order to remove the batch iteration loop, let's consider the complete set of computations we need to perform to complete a single epoch. Here they are for batch one:</p>

<p><center>
<a href="images/computation-batch1.svg">
<img src="images/computation-batch1.svg" width="100%" url="images/computation-batch1.svg">
</a>
</center>
</p>

<p>and batch two:</p>

<p><center>
<a href="images/computation-batch2.svg">
<img src="images/computation-batch2.svg" width="100%" url="images/computation-batch2.svg">
</a>
</center>
</p>

<p>We've been processing the input word by word, even in the batched non-vectorized case. Graphically, that means we process from left to right all of the computations for <i>cat</i> then move onto the next word, <i>chat</i> etc... This makes sense because there is a dependency between the partial <span class=inlinecode>h</span>  results. Recall that our recurrence relation associated with the RNN is <img style="vertical-align: -2.8455pt;" src="images/eqn-F224DE1728AA3CC4724E5AE9C5429D65-depth002.71.svg"> and then we run the last <span class=eqn>h</span> into <img style="vertical-align: -0.5pt;" src="images/eqn-80D29D02FAD95C4A1D659564BE6CA9B4-depth000.22.svg"> to get the output vector holding the predicted probabilities of the target languages.  Here's a depiction of the dependencies for the <i>cat</i> computations:	</p>

<p><center>
<a href="images/computation-depends.svg">
<img src="images/computation-depends.svg" width="100%" url="images/computation-depends.svg">
</a>
</center>
</p>

<p>To get the right answer for each word, that means computation must proceed left to right, one <span class=eqn>h</span> computation after the other.  That dependency, however, only exists within a word between time <img style="vertical-align: -1.08pt;" src="images/eqn-F3E90CE87A25538F5A4BE79A0A7C0FA5-depth001.08.svg"> and <span class=eqn>t</span>. So, we are free to process all words within a batch simultaneously, as long as we respect the time dependency. In other words, we process all characters at the same time position, <span class=eqn>t</span>, simultaneously for all words in the batch. Here's the first batch of computations again, this time with blue boxes around the computations at time <span class=eqn>t</span> and dashed arrows indicating the sequence of operations:</p>

<p><center>
<a href="images/computation-batch1-vector.svg">
<img src="images/computation-batch1-vector.svg" width="100%" url="images/computation-batch1-vector.svg">
</a>
</center>
</p>

<p>First, we compute <img style="vertical-align: -2.0475pt;" src="images/eqn-DE8465D723217241A3096B8763F598FE-depth001.95.svg"> for the first letter, &ldquo;c&rdquo;, of both <i>cat</i> and <i>chat</i>. Next, we compute <img style="vertical-align: -2.0475pt;" src="images/eqn-096CB9D94C653880BE7EF740C2FB0DBD-depth001.95.svg"> based upon <img style="vertical-align: -2.0475pt;" src="images/eqn-DE8465D723217241A3096B8763F598FE-depth001.95.svg"> and the second characters, &ldquo;a&rdquo; and &ldquo;h&rdquo;, and so on. The only wrinkle is what happens at the fourth character because <i>cat</i> only has three letters, but we'll deal with that later. For now, let's pretend that all words are the same length and figure out how to process the components of a batch.</p>



<h3 id="sec:2.2.1">2.2.1 Processing batches, one character position at a time</h3>


<p>The biggest difference from the non-vectorized minibatch version is that we need to track a different <span class=inlinecode>h</span> for each word in the batch simultaneously; remember that the <span class=inlinecode>h</span> vector is a representation in some high dimensional space for a specific word. If we used a single <span class=inlinecode>h</span> vector, we would get a muddied vector that tried to represent all words.  For convenience, let's pack all of the <span class=inlinecode>h</span> vectors as columns in a matrix called <span class=inlinecode>H</span>. Then, <span class=inlinecode>W@H</span> is matrix-by-matrix multiplication, yielding a matrix rather than a vector, as it did in the non-vectorized version. </p>

<p>Similarly, to simultaneously process the character vectors from all words at step <span class=eqn>t</span>, we multiply <span class=eqn>U</span> by batch input matrix <span class=eqn>B</span>. <span class=eqn>B</span> has character vectors as columns, one for each batch word.  Constructing matrix <span class=eqn>B</span> requires further explanation, let's do that after looking at the code; for now, just assume <span class=eqn>B</span> has <span class=inlinecode>len(vocab)</span> rows and batch size columns.</p>
<div class=aside><b>Performing multiple matrix-vector multiplications is matrix-matrix multiplication</b><br>

<p>Imagine that I want to apply <span class=eqn>W</span> to three vectors <span class=eqn>a</span>, <span class=eqn>b</span>, and <span class=eqn>c</span>. That's the same as combining those three vectors as columns in a matrix and applying <span class=eqn>W</span>:</p>

<p><center>
<a href="images/combine-matrix-vector.svg">
<img src="images/combine-matrix-vector.svg" width="42%" url="images/combine-matrix-vector.svg">
</a>
</center>
</p>

<p>The resulting matrix  has columns <span class=eqn>Wa</span>, <span class=eqn>Wb</span>, and <span class=eqn>Wc</span>.</p>

</div>
<p>The RNN model with <span class=inlinecode>H</span> as a matrix and batch input matrix <span class=inlinecode>B</span> looks like this:</p>

<p><center>
<a href="images/equation-vec-W-U-V.svg">
<img src="images/equation-vec-W-U-V.svg" width="63%" url="images/equation-vec-W-U-V.svg">
</a>
</center>
</p>

<p>The output is now also a matrix, not a vector, and each column represents the output vector for a specific word in the batch.  (Normally, I would capitalize <span class=eqn>o</span> to indicate it's a matrix, but that looks too much like a zero so I'm leaving it is lowercase.) Because I like the first dimension to represent the different input records, I take the transpose of output matrix <span class=eqn>o</span> so that it has batch size rows and columns for each target class probability.  Ok, enough pictures. Let's look at the code differences in the SGD loop:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="44%" align="center">Non-vectorized minibatch SGD loop</th><th valign=top align="center">Vectorized minibatch SGD loop</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">

<center>
<center>
<a href="images/batch-side-by-side-vec-A.png">
<img src="images/batch-side-by-side-vec-A.png" width="100%" url="images/batch-side-by-side-vec-A.png">
</a>
</center>

</center>

</td><td valign=top align="center">

<center>
<center>
<a href="images/batch-side-by-side-vec-B.png">
<img src="images/batch-side-by-side-vec-B.png" width="100%" url="images/batch-side-by-side-vec-B.png">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>	
<p>Instead of processing batches word by word, now, we grab the entire batch as matrix <span class=inlinecode>batch_X</span> then use <span class=inlinecode>batch_X[:,t]</span> to extract the character vectors at a particular step <span class=inlinecode>t</span>. The <span class=inlinecode>cross_entropy()</span> function computes the loss for each word in the batch by comparing row <span class=inlinecode>i</span> of output matrix <span class=eqn>o</span> with the correct target in <span class=inlinecode>batch_y[i]</span>. The function return returns the average loss across all words in the batch.</p>
<div class=aside><b>What exactly is vectorization?</b><br>

<p>Converting a loop to a vector operation is tricky until you get the hang of it, so let's make the term vectorization more concrete. Imagine that we wanted to add two lists of numbers (vectors) <span class=inlinecode>a</span> and <span class=inlinecode>b</span> to get <span class=inlinecode>c</span>:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top align="center">Loop implementation</th><th valign=top width="55%" align="center">Addition of two vectors</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">

<center>
<center>
<a href="images/a-plus-b.png">
<img src="images/a-plus-b.png" width="30%" url="images/a-plus-b.png">
</a>
</center>

</center>

</td><td valign=top align="left">

<div class="codeblk">for i in range(len(a)):
    c[i] = a[i] + b[i]</div>

</td>
</tr>
</tbody>
</table>
</center>	
<p>If those vectors were tensors (or <span class=inlinecode>numpy</span> arrays), we could use just <span class=inlinecode>c = a + b</span>, thanks to Python operator overloading.  In fact, we can even perform a vector operation on a slice of the vectors: <span class=inlinecode>a[2:]+b[2:]</span> is <span class=inlinecode>[9, 3]</span>. For our purposes, that's all there is to vectorization.</p>

<p>Vector operations are usually easier to type than loops but, more importantly, vector operations execute more quickly than Python loops.  This is because either the vector operations are implemented in C++, and C++ is dramatically faster than Python at looping, or implemented in Cuda that runs in parallel on the GPU. You can think of vectorization as a means to tell PyTorch or other tensor library what can be computed simultaneously.</p>

</div>
<p>We know that <span class=inlinecode>X_train</span> is a list of lists containing the list of characters for each word in the training set. <span class=inlinecode>X_train[p:p+batch_size]</span>, therefore, just grabs the lists for words in the current batch, such as the first batch:</p>


<div class="codeblk">[['c', 'a', 't'],
 ['c', 'h', 'a', 't']]</div>


<p>The only mysterious part is  what we are doing with the <span class=inlinecode>onehot_matrix()</span> function, which leads us to the final important implementation detail for vectorized minibatching.</p>



<h3 id="sec:2.2.2">2.2.2 Padding short words with 0 vectors on the left</h3>


<p>To figure out the appropriate data structure to represent our training set during computation, let's take a look at all character vectors associated with the complete training set:</p>

<p><center>
<a href="images/all-words.svg">
<img src="images/all-words.svg" width="50%" url="images/all-words.svg">
</a>
</center>
</p>

<p>The most convenient single entity that aggregates all of those vectors is a three-dimensional matrix. Each row will represent a word, each column will represent a character, and the final dimension (going into the screen) will represent the one-hot encoding. To create that matrix, flip the column one-hot vectors representing characters so they are going back into the screen, then stack them up, one on top of the other. Because the words are of different length, and tensors all need to have the same length, we need to pad missing characters with zero one-hot vectors up to the max word length. The result is the right-padded 3-D matrix you see here on the left:</p>

<p>	</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top align="center">Right-padded one-hot matrix</th><th valign=top align="center">Left-padded one-hot matrix</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">

<center>
<center>
<a href="images/3D-matrix.png">
<img src="images/3D-matrix.png" width="45%" url="images/3D-matrix.png">
</a>
</center>

</center>

</td><td valign=top align="center">

<center>
<center>
<a href="images/3D-matrix-right.png">
<img src="images/3D-matrix-right.png" width="45%" url="images/3D-matrix-right.png">
</a>
</center>

</center>

</td>
</tr>
</tbody>
</table>
</center>
<p>Unfortunately padding on the right screws up our computation. To see how, let's look at the recurrence relation for a single one-hot vector again, ignoring the nonlinearity as is not relevant to this padding discussion:</p>

<div><img class="blkeqn" src="images/blkeqn-F224DE1728AA3CC4724E5AE9C5429D65.svg" alt="" width="" url="images/blkeqn-F224DE1728AA3CC4724E5AE9C5429D65.svg"></div>

<p>After the final letter in <i>cat</i>, my initial reaction was that <span class=eqn>x<sub>t</sub></span> as the zero vector would not alter the <span class=eqn>h<sub>t</sub></span> vector, so padding on the right wouldn't be a problem. It turns out that's not correct. If we make <img style="vertical-align: -2.0475pt;" src="images/eqn-02D39FB1D0491A68BF66D3609C171DE7-depth001.95.svg"> and <img style="vertical-align: -2.1944997pt;" src="images/eqn-407DD06C43056B35E7A1FCA2F47C8716-depth002.09.svg"> zero vectors, the recurrence relation ends up applying <span class=eqn>W</span> to <img style="vertical-align: -2.1944997pt;" src="images/eqn-9B3C2A82E196EF44D1ADF9E2D25F59B4-depth002.09.svg"> twice, but we want <img style="vertical-align: -2.1944997pt;" src="images/eqn-9B3C2A82E196EF44D1ADF9E2D25F59B4-depth002.09.svg"> to stay the same at steps 4 and 4.  Notice that <img style="vertical-align: -2.0475pt;" src="images/eqn-0E72DDA469B0E69153F2D21C1339B86B-depth001.95.svg"> would not be the same thing as <img style="vertical-align: -2.1944997pt;" src="images/eqn-9B3C2A82E196EF44D1ADF9E2D25F59B4-depth002.09.svg">:</p>

<div><img class="blkeqn" src="images/blkeqn-CE0477A53EF4CBAD5CECD7C146255F02.svg" alt="" width="" url="images/blkeqn-CE0477A53EF4CBAD5CECD7C146255F02.svg"></div>

<p>The way around this  is simple but looks weird: we left pad the matrix instead of right padding. See the diagram above on the right.  This is how left padding affects the recurrence relation for <i>cat</i>:</p>

<div><img class="blkeqn" src="images/blkeqn-62C1D1FC2DFDAC40147DEB20CD9B803B.svg" alt="" width="" url="images/blkeqn-62C1D1FC2DFDAC40147DEB20CD9B803B.svg"></div>

<p>where one-hot vectors for the letters of <i>cat</i> are <img style="vertical-align: -2.1944997pt;" src="images/eqn-28C5EAC946471F68EEFB01F7A53B1844-depth002.09.svg">, <img style="vertical-align: -2.0475pt;" src="images/eqn-02D39FB1D0491A68BF66D3609C171DE7-depth001.95.svg">, and <img style="vertical-align: -2.1944997pt;" src="images/eqn-407DD06C43056B35E7A1FCA2F47C8716-depth002.09.svg">. Because <img style="vertical-align: -2.1944997pt;" src="images/eqn-91923B083A33516B2AE173D6A93C2573-depth002.09.svg"> starts out as the zero vector and all of the input vectors are zero, <img style="vertical-align: -2.1944997pt;" src="images/eqn-91923B083A33516B2AE173D6A93C2573-depth002.09.svg">, <img style="vertical-align: -2.0475pt;" src="images/eqn-DE8465D723217241A3096B8763F598FE-depth001.95.svg">, and <img style="vertical-align: -2.0475pt;" src="images/eqn-096CB9D94C653880BE7EF740C2FB0DBD-depth001.95.svg"> are all the zero vector as well. </p>

<p>So, the <span class=inlinecode>onehot_matrix()</span> function takes a batch or the entire training set and converts it to a 3D matrix that aggregates all of the character one-hot vectors for all of the words.  Returning to the vectorized SGD loop in the previous subsection, let's visualize what's happening with the following statement:</p>


<div class="codeblk">x_step_t = batch_X[:,t].T # make it len(vocab) x batch_size</div>


<p>The <span class=inlinecode>:</span> in <span class=inlinecode>batch_X[:,t]</span> spans all words in the batch and the <span class=inlinecode>t</span> refers to the step. So, this gives us all characters at step <span class=inlinecode>t</span> across all words. Since we did not mention the third dimension, the one-hot vector dimension, we get the entire one-hot vector for each character position. The resulting 2D matrix has dimensions batch size by vocabulary length, which means that the one-hot vectors are horizontal not vertical.  For the matrix algebra to work out, we need those one-hot vectors within the matrix to be vertical as we've done in the past. That's why we take the need the transpose, <span class=inlinecode>batch_X[:,t].T</span>.</p>

<p>And there you have it. Those are all the details I had to figure out in order to implement an efficient RNN using matrices instead of neural network layers as raw components.  Our initial  non-vectorized SGD training loop, which updated model parameters after computing loss for each word, encompasses all of the RNN key ideas. For a realistic RNN, however, only the vectorized version is fast enough. The vectorized version requires minibatching and  all of the machinery required to get the 3D padded batch matrix set up properly. As usual, the theory is straightforward but actually building a practical RNN requires a much deeper understanding of the details. </p>

<p>You can check out the full implementation in the <a href="https://github.com/parrt/ml-articles/blob/master/rnn/notebooks/vectorized.ipynb">vectorized minibatch SGD notebook</a> and the similar <a href="https://github.com/parrt/ml-articles/blob/master/rnn/notebooks/gpu.ipynb">GPU-enabled vectorized minibatch SGD notebook</a>.</p>




</body>
</html>