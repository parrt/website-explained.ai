<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118361649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118361649-1');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
<title>Clarifying exceptions and visualizing tensor operations in deep learning code</title>
<!-- META -->
<!-- LinkedIn meta -->
<meta property='og:title' content="Clarifying exceptions and visualizing tensor operations in deep learning code"/>
<meta property='og:image' content="">
<meta property='og:description' content=""/>
<meta property='og:url' content="http://explained.ai/tensor-sensor/index.html"/>

<!-- Facebook meta -->
<meta property="og:type" content="article" />

<!-- Twitter meta -->
<meta name="twitter:title" content="Clarifying exceptions and visualizing tensor operations in deep learning code">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@the_antlr_guy">
<meta name="twitter:creator" content="@the_antlr_guy">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="">
<!-- END META -->
</head>
<body>
<div class="watermark">
<i>Brought to you by <a href='http://explained.ai'>explained.ai</a></i><br>
</div>

<h1>1 Clarifying exceptions and visualizing tensor operations in deep learning code</h1>

<p></p>

<p><a href="http://parrt.cs.usfca.edu">Terence Parr</a></p>

<p style="font-size: 80%; line-height:1.1;">(Terence teaches in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a>. You might know Terence as the creator of the ANTLR parser generator.)</p>



<div id="toc">
<p class="toc_title">Contents</p>
<ul>
	<li><a href="#sec:1.1">Isolating issues in tensor code is maddening!</a>
	<ul>
			<li><a href="#sec:1.1.1">Debugging a simple linear layer</a></li>
			<li><a href="#sec:1.1.2">Debugging a complex tensor expression</a></li>
			<li><a href="#sec:1.1.3">Clarifying exceptions triggered within prebuilt layers</a></li>

	</ul>
	</li>
	<li><a href="#sec:1.2">Clarifying deeply-buried tensor code</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.3">Explaining tensor code without triggering exceptions</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.4">Visualizing 3D tensors and beyond</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.5">Visualizing subexpression tensor shapes</a>
	<ul>
	</ul>
	</li>
	<li><a href="#sec:1.6">Key TensorSensor implementation Kung Fu</a>
	<ul>
	</ul>
	</li>

</ul>
</div>


<p>Most people solve deep learning problems using high-level libraries such as <a href="https://keras.io/">Keras</a> or <a href="https://www.fast.ai/">fastai</a>,  which makes sense. These libraries hide a lot of implementation details that we either don't care about or can learn later.  To truly understand deep learning, however, I think it's important at some point to implement your own network layers and training loops. For example, see my recent article called <a href="https://explained.ai/rnn/index.html">Explaining RNNs without neural networks</a>. If you're comfortable building deep learning models while leaving some of the details a bit fuzzy, then this article is not for you.  In my quirky case, I care more about learning something deeply than actually applying it to something useful, so I go straight for the details. (I guess that's why I work at a university, not an industry ðŸ˜€.)  This article is in response to a pain point I experienced during an obsessive coding and learning burn through the fundamentals of deep learning in the isolation of Covid summer 2020.</p>

<p><center>
<a href="images/teaser.png">
<img src="images/teaser.png" width="50%" align="right" url="images/teaser.png">
</a>
</center>
One of the biggest challenges when writing code to implement deep learning networks, particularly for us newbies, is getting all of the tensor (matrix and vector) dimensions to line up properly. It's really easy to lose track of tensor dimensionality in complicated expressions involving multiple tensors and tensor operations. When you ask for improper computations, you're going to run into some less than helpful exception messages.  To help myself and other programmers debug tensor code, I built a new library called <a href="https://github.com/parrt/tensor-sensor">TensorSensor</a> (<span class=inlinecode>pip install tensor-sensor</span>).  TensorSensor clarifies exceptions by augmenting messages and visualizing Python code to indicate the shape of tensor variables. It works with <a href="https://www.tensorflow.org/">Tensorflow</a>, <a href="https://pytorch.org/">PyTorch</a>, and <a href="https://numpy.org/">Numpy</a>, as well as higher-level libraries like <a href="https://keras.io/">Keras</a> and <a href="https://www.fast.ai/">fastai</a>.</p>



<h2 id="sec:1.1">1.1 Isolating issues in tensor code is maddening!</h2>


<p>Even for experts, it can be hard to quickly identify the cause of an exception in a line of Python code performing tensor operations.  The debugging process usually involves adding a print statement in front of the offending line to emit the shape of each tensor operand.  That works requires editing the code to create the debugging statement and rerunning the training process. Or, we can manually click or type commands to request all operand shapes using an interactive debugger. (This can be less practical in an IDE like PyCharm where executing code in debug mode seems to be much slower.)  The following subsections illustrate the anemic default exception messages and my proposed TensorSensor approach, rather than a debugger or print statements.</p>



<h3 id="sec:1.1.1">1.1.1 Debugging a simple linear layer</h3>


<p>Let's look at a simple tensor computation to illustrate the less-than-optimal information provided by the default exception message. Consider the following simple NumPy implementation for a hardcoded single (linear) network layer that contains a tensor dimension error.</p>


<div class="codeblk">import numpy as np

n = 200                          # number of instances
d = 764                          # number of instance features
n_neurons = 100                  # how many neurons in this layer?

W = np.random.rand(d,n_neurons)  # Ooops! Should be (n_neurons,d) &lt;=======
b = np.random.rand(n_neurons,1)
X = np.random.rand(n,d)          # fake input matrix with n rows of d-dimensions

Y = W @ X.T + b                  # pass all X instances through layer</div>


<p>Executing that code triggers an exception whose important elements are:</p>

<p>
<div class=exception>...
---> 10 Y = W @ X.T + b
	
ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 764 is different from 100)</div>
</p>

<p>The exception identifies the offending line and which operation (<span class=inlinecode>matmul</span>: matrix multiply) but would be more useful if it gave the complete tensor dimensions. Also, the exception would be unable to distinguish between multiple matrix multiplications occurring in one line of Python.</p>

<p>Next, let's see how TensorSensor makes debugging that statement much easier. If we wrap the statement using a Python <span class=inlinecode>with</span> statement and <span class=inlinecode>tsensor</span>'s <span class=inlinecode>clarify()</span>, we get a visualization and an augmented error message. </p>


<div class="codeblk">import tsensor
with tsensor.clarify():
    Y = W @ X.T + b</div>


<p>
<a href="images/numpy-mm-py.svg">
<img nocenter="true" src="images/numpy-mm-py.svg" url="images/numpy-mm-py.svg">
</a>
</p>

<p>
<div class=exception>...
ValueError: matmul: Input operand ...
Cause: @ on tensor operand W w/shape (764, 100) and operand X.T w/shape (764, 200)</div>
</p>

<p>It's clear from the visualization that <span class=inlinecode>W</span>'s dimensions should be flipped to be <span class=inlinecode>n_neurons x d</span>; the columns of <span class=inlinecode>W</span> must match the rows of <span class=inlinecode>X.T</span>. You can also checkout a <a href="images/numpy-mm.png">complete side-by-side image</a> with and without <span class=inlinecode>clarify()</span> to see what it looks like in a notebook.</p>

<p>The <span class=inlinecode>clarify()</span> functionality incurs no overhead on the executing program until an exception occurs. Upon exception, <span class=inlinecode>clarify()</span>:</p>
<ol>
<li> Augments the exception object's message created by the underlying tensor library.</li>
<li> Gives a visual representation of the tensor sizes involved in the offending operation; only the operands and operator involved in the exception are highlighted, while the other Python elements are de-highlighted.</li>
</ol>
<p>TensorSensor also clarifies tensor-related exceptions raised by PyTorch and TensorFlow. Here are the equivalent code snippets and resulting augmented exception error messages (<span class=inlinecode>Cause: @ on tensor ...</span>) and visualization from TensorSensor:</p>
<center>
<table style="">
<thead>
<tr>
<th valign=top width="50%" align="center">PyTorch</th><th valign=top align="center">TensorFlow</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="left">

<div class="codeblk">import torch
W = torch.rand(d,n_neurons)
b = torch.rand(n_neurons,1)
X = torch.rand(n,d)
with tsensor.clarify():
    Y = W @ X.T + b</div>

</td><td valign=top align="left">

<div class="codeblk">import tensorflow as tf
W = tf.random.uniform((d,n_neurons))
b = tf.random.uniform((n_neurons,1))
X = tf.random.uniform((n,d))
with tsensor.clarify():
    Y = W @ tf.transpose(X) + b</div>

</td>
</tr>
<tr>
<td valign=top align="left">

<a href="images/mm.svg">
<img nocenter="true" src="images/mm.svg" url="images/mm.svg">
</a>



<div class=exception>RuntimeError: size mismatch, m1: [764 x 100], m2: [764 x 200] at /tmp/pip-req-build-as628lz5/aten/src/TH/generic/THTensorMath.cpp:41
Cause: @ on tensor operand W w/shape [764, 100] and operand X.T w/shape [764, 200]</div>

<td>
<img src="images/tf-mm.svg" nocenter="true">
<html>
<div class=exception>InvalidArgumentError: Matrix size-incompatible: In[0]: [764,100], In[1]: [764,200] [Op:MatMul]
Cause: @ on tensor operand W w/shape (764, 100) and operand tf.transpose(X) w/shape (764, 200)</div>
</td>
</tr>
</tbody>
</table>
</center>
<p>The PyTorch message does not identify which operation triggered the exception, but TensorFlow's message does indicate matrix multiplication. Both show the operand dimensions. These default exception messages are probably good enough for this simple tensor expression for a linear layer. Still, it's easier to see the problem with the TensorSensor visualization.</p>

<p>You might be wondering, though, why tensor libraries don't generate a more helpful exception message that identified the names of the Python variables involved in the offending subexpression.  It's not that the library authors couldn't be bothered. The fundamental issue is that Python tensor libraries are wrappers around extremely efficient cores written in C or C++. Python passes, say, the data for two tensors to a C++ function, but not the associated tensor variable names in Python space. An exception caught deep in C++ has no access to the local and global variable spaces in Python, so it just throws a generic exception back over the fence.  Because Python traps exceptions at the statement level, it also cannot isolate the subexpression within the statement.  (To learn how TensorSensor manages to generate such specific messages, check out the section below.)</p>



<h3 id="sec:1.1.2">1.1.2 Debugging a complex tensor expression</h3>


<p>That lack of specificity in default messages makes it hard to identify bad subexpressions within more complicated statements that contain lots of operators. For example, here's a statement pulled from the guts of a Gated Recurrent Unit (GRU) implementation:</p>


<div class="codeblk">h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)</div>


<p>It doesn't matter what it's computing or what the variables represent, just that they are tensor variables. There are two matrix multiplications, two vector additions, and even a vector element-wise modification (<span class=inlinecode>r*h</span>).  Without augmented error messages or visualizations we wouldn't know which operator and operands caused an exception. To demonstrate how TensorSensor clarifies exceptions in this case, we need to give some fake definitions for the variables used in the statement (the assignment to <span class=inlinecode>h_</span>) to get executable code:</p>


<div class="codeblk">nhidden = 256
Whh_ = torch.eye(nhidden, nhidden)  # Identity matrix
Uxh_ = torch.randn(d, nhidden)
bh_  = torch.zeros(nhidden, 1)
h = torch.randn(nhidden, 1)         # fake previous hidden state h
r = torch.randn(nhidden, 1)         # fake this computation
X = torch.rand(n,d)                 # fake input

with tsensor.clarify():
    h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)</div>


<p>Again, you can ignore the actual computation performed by the code to focus on the shape of the tensor variables.  </p>

<p>For most of us, it's impossible to identify the problem just by looking at the tensor dimensions and the tensor code.  The default exception message is helpful of course, but most of us will still struggle to identify the problem.  Here are the key bits of the default exception message (note the less-than-helpful reference to the C++ code):</p>

<p>
<div class=exception>---> 10     h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)
RuntimeError: size mismatch, m1: [764 x 256], m2: [764 x 200] at /tmp/pip-req-build-as628lz5/aten/src/TH/generic/THTensorMath.cpp:41
</div>
</p>

<p>What we need to know is which operator and operands failed, then we can look at the dimensions to identify the problem.  Here is TensorSensor's visualization and augmented exception message:</p>

<p>
<a href="images/torch-gru.svg">
<img nocenter="true" src="images/torch-gru.svg" url="images/torch-gru.svg">
</a>

<div class=exception>---> 10 h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)
RuntimeError: size mismatch, m1: [764 x 256], m2: [764 x 200] at /tmp/pip-req-build-as628lz5/aten/src/TH/generic/THTensorMath.cpp:41
Cause: @ on tensor operand Uxh_ w/shape [764, 256] and operand X.T w/shape [764, 200]
</div>
</p>

<p>The human eye quickly latches onto the indicated operator and the dimensions on the matrix-matrix multiply. Ooops: The columns of <span class=inlinecode>Uxh_</span> must match the rows of <span class=inlinecode>X.T</span>. <span class=inlinecode>Uxh_</span> has its dimensions flipped and should be:</p>


<div class="codeblk">Uxh_ = torch.randn(nhidden, d)</div>


<p>At this point, we've only used our own tensor computations specified directly within the <span class=inlinecode>with</span> code block. What about exceptions triggered within a tensor library's prebuilt network layer?</p>



<h3 id="sec:1.1.3">1.1.3 Clarifying exceptions triggered within prebuilt layers</h3>


<p>TensorSensor visualizes the last piece of code before it enters your chosen tensor library. For example, let's use the standard PyTorch <span class=inlinecode>nn.Linear</span> linear layer but pass in an <span class=inlinecode>X</span> matrix that is <span class=inlinecode>n x n</span>, instead of the proper <span class=inlinecode>n x d</span>:</p>


<div class="codeblk">L = torch.nn.Linear(d, n_neurons)
X = torch.rand(n,n) # oops! Should be n x d
with tsensor.clarify():
    Y = L(X)</div>

<center>
<table style="">
<thead>
<tr>
<th valign=top width="30%" align="center">Visualization</th><th valign=top align="center">Augmented exception message</th>
</tr>
</thead>
<tbody>
<tr>
<td valign=top align="center">

<a href="images/torch-nn-linear.svg">
<img nocenter="true" src="images/torch-nn-linear.svg" url="images/torch-nn-linear.svg">
</a>


</td><td valign=top align="center">
<div class=exception>RuntimeError: size mismatch, m1: [200 x 200], m2: [764 x 100] at /tmp/pip-req-build-as628lz5/aten/src/TH/generic/THTensorMath.cpp:41
Cause: L(X) tensor arg X w/shape [200, 200]</div>
</td>
</tr>
</tbody>
</table>
</center>
<p>TensorSensor treats calls into tensor libraries as operators, whether the call is to a network layer or something simple like <span class=inlinecode>torch.dot(a,b)</span>. Exceptions triggered within library functions yield messages that identify the function and dimensionality of any tensor arguments.</p>




<h2 id="sec:1.2">1.2 Clarifying deeply-buried tensor code</h2>


<p>When using a high-level library like Keras with pre-built layer objects, we get excellent error messages that indicate a mismatch in dimensionality between the layers of deep learning network. If you're building a custom layer, however, or just implementing your own to understand deep learning more thoroughly, you'll need to examine exceptions triggered inside your layer objects. TensorSensor descends into any code initiated from within the <span class=inlinecode>with</span> statement block, stopping only when it reaches a tensor library function.</p>

<p>As a demonstration, let's create our own linear network layer by wrapping the simple linear layer code from above in a class definition:</p>


<div class="codeblk">class Linear:
    def __init__(self, d, n_neurons):
        self.W = torch.randn(n_neurons, d)
        self.b = torch.zeros(n_neurons, 1)
    def __call__(self, input):
        return self.W @ input + self.b</div>


<p>Then, we can create a layer as an object and perform a forward computation using some fake input <span class=inlinecode>X</span>:</p>


<div class="codeblk">L = Linear(d, n_neurons) # create a layer
X = torch.rand(n, d)     # fake input
with tsensor.clarify():
    Y = L(X)             # L(X) invokes L.__call__()</div>


<p>The <span class=inlinecode>Linear</span> layer has the correct dimensionality on weights <span class=inlinecode>W</span> and bias <span class=inlinecode>b</span>, but the equation in <span class=inlinecode>__call__()</span> incorrectly references <span class=inlinecode>input</span> rather than the transpose of that input matrix, triggering an exception:</p>

<p>
<a href="images/torch-linear.svg">
<img nocenter="true" src="images/torch-linear.svg" url="images/torch-linear.svg">
</a>

<div class=exception>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-20-b6b1bd407c61> in &lt;module>
      9 
     10 with tsensor.clarify(hush_errors=False):
---> 11     Y = L(X)

&lt;ipython-input-16-678a8372f1c2> in __call__(self, x)
      4         self.b = torch.zeros(n_neurons, 1)
      5     def __call__(self, x):
----> 6         return self.W@x + self.b    # L(X) invokes L.__call__()

RuntimeError: size mismatch, m1: [100 x 764], m2: [200 x 764] at /tmp/pip-req-build-as628lz5/aten/src/TH/generic/THTensorMath.cpp:41
Cause: @ on tensor operand self.W w/shape [100, 764] and operand x w/shape [200, 764]</div>
</p>

<p>Because <span class=inlinecode>L(X)</span> invokes our own code, not a tensor library function, TensorSensor clarifies the offending statement in <span class=inlinecode>__call__()</span> rather than the <span class=inlinecode>Y=L(X)</span> statement within the <span class=inlinecode>with</span> block.</p>

<p>So far we've focused on clarifying exceptions, but sometimes we simply want to explain some correct tensor code to others or to make it easier to read. It's also the case that not all erroneous code triggers an exception; sometimes, we simply get the wrong answer. If that wrong answer has the wrong shape, TensorSensor can help.  Next, let's look at TensorSensor's <span class=inlinecode>explain()</span> functionality.</p>



<h2 id="sec:1.3">1.3 Explaining tensor code without triggering exceptions</h2>


<p>TensorSensor's <span class=inlinecode>clarify()</span> has no effect unless tensor code triggers an exception. To visualize tensor dimensionality within exception-free Python statements, TensorSensor provides a mechanism called <span class=inlinecode>explain()</span> that is similar to <span class=inlinecode>clarify()</span>, except that <span class=inlinecode>explain()</span> generates a visualization for each statement executed within the block. For example, here's our familiar linear layer computation again but wrapped in an <span class=inlinecode>explain()</span> block and with resulting tensor shape visualization:</p>


<div class="codeblk">n = 200         # number of instances
d = 764         # number of instance features
n_neurons = 100 # how many neurons in this layer?

W = torch.rand(n_neurons,d)
b = torch.rand(n_neurons,1)
X = torch.rand(n,d)

with tsensor.explain():
    Y = W @ X.T + b</div>


<p>
<a href="images/torch-mm-explain.svg">
<img nocenter="true" src="images/torch-mm-explain.svg" url="images/torch-mm-explain.svg">
</a>
</p>

<p>It's handy to see the shape of the resulting computation (for <span class=inlinecode>Y</span>). Plus, notice that column vector <span class=inlinecode>b</span> is easily identified visually as a  vertical rectangle (a matrix with a single column). Row vectors are horizontal rectangles:</p>


<div class="codeblk">n_neurons = 100
b = torch.rand(1,n_neurons) # 2D tensor 1 x n_neurons
with tsensor.explain() as c:
    y = b @ b.T</div>



<a href="images/torch-bb.svg">
<img nocenter="true" src="images/torch-bb.svg" url="images/torch-bb.svg">
</a>



<p>Column and row vectors are still 2D matrices, but we can also have 1D tensors:</p>


<div class="codeblk">b = torch.rand(n_neurons) # 1D tensor
with tsensor.explain() as c:
    y = torch.dot(b, b)</div>


<p>
<a href="images/torch-dot.svg">
<img nocenter="true" src="images/torch-dot.svg" url="images/torch-dot.svg">
</a>
</p>

<p>The visualization for 1D tensors look like 2D row vectors but the yellow color (versus pale green) signals that it's 1D. (This is important because tensor libraries often treat 1D vectors and 2D row vectors differently.) The result (<span class=inlinecode>y</span>) is a scalar and, hence, has no visualization component.</p>

<p>Before moving on to 3D+ tensor visualizations, there are some important characteristics of <span class=inlinecode>explain()</span> to highlight.  First, unlike <span class=inlinecode>clarify()</span>, <span class=inlinecode>explain()</span> does not descend into code invoked from the statements in the <span class=inlinecode>with</span> block.  It only visualizes statements within the <span class=inlinecode>with</span> block proper. Every execution of the <span class=inlinecode>with</span> block statement(s) generates a new image and at most one image. In other words, if the <span class=inlinecode>with</span> code is in a loop, you  won't see multiple visualizations for the same statement.</p>

<p>Second, both <span class=inlinecode>clarify()</span> and <span class=inlinecode>explain()</span> cause tensor statements to execute twice, so beware of side effects. For example, the following code prints &ldquo;hi&rdquo; twice, once by TensorSensor and once during normal program execution. Most tensor expressions are side-effect free, so it's usually not a problem, but keep it in mind. (See the implementation section below for more details.)</p>


<div class="codeblk">with tsensor.explain():
    print("hi")</div>


<p>Finally, because <span class=inlinecode>explain()</span> creates a visualization for every statement in the <span class=inlinecode>with</span> block, &ldquo;explained&rdquo; code will run significantly slower. </p>



<h2 id="sec:1.4">1.4 Visualizing 3D tensors and beyond</h2>


<p>Understanding and debugging code that uses tensors beyond two dimensions can be really challenging.  Unfortunately, this comes up a lot. For example, it's very common to train deep learning networks in batches for performance reasons.  That means reshaping the input matrix, <span class=inlinecode>X</span>, into <span class=inlinecode>n_batches x batch_size x d</span> rather than <span class=inlinecode>n x d</span>. The following code simulates passing multiple batches through a linear layer.</p>


<div class="codeblk">n = 200                          # number of instances
d = 764                          # number of instance features
n_neurons = 100                  # how many neurons in this layer?
batch_size = 10                  # how many records per batch?
n_batches = n // batch_size

W = torch.rand(n_neurons,d)
b = torch.rand(n_neurons,1)
X = torch.rand(n_batches,batch_size,d)

with tsensor.explain():
    for i in range(n_batches):
        batch = X[i,:,:]
        Y = torch.relu(W @ batch.T + b)</div>


<p>Here's how TensorSensor visualizes the two statements (despite being in a loop, each visualization is given once):</p>

<p>
<a href="images/torch-batch-1.svg">
<img nocenter="true" src="images/torch-batch-1.svg" url="images/torch-batch-1.svg">
</a>
</p>

<p>
<a href="images/torch-batch-2.svg">
<img nocenter="true" src="images/torch-batch-2.svg" url="images/torch-batch-2.svg">
</a>
</p>

<p>To represent the shape of 3D tensors, such as <span class=inlinecode>X</span>, TensorSensor draws an extra box to simulate a three-dimensional perspective, and gives the third dimension at 45 degrees.</p>

<p>To visualize tensors beyond 3D, let's consider an example where input instances  are images containing red, green, blue values for each input pixel. A common representation would be a <span class=inlinecode>1 x d x 3</span> matrix for each image (<span class=inlinecode>d</span> is the width times height of the image). For <span class=inlinecode>n</span> images, we have an <span class=inlinecode>n x d x 3</span> matrix. Add batching to that and we get a 4D <span class=inlinecode>n_batches x batch_size x d x 3</span> matrix:</p>


<div class="codeblk">X = torch.rand(n_batches,batch_size,d,3)
with tsensor.explain():
    batch = X[i,:,:]</div>


<p>
<a href="images/torch-4D.svg">
<img nocenter="true" src="images/torch-4D.svg" url="images/torch-4D.svg">
</a>
</p>

<p>As you can see, dimensions beyond three are shown at the bottom of the 3D representation preceded by an ellipsis. In this case, the fourth dimension is displayed as &ldquo;<span class=inlinecode>...x3</span>&rdquo;.</p>



<h2 id="sec:1.5">1.5 Visualizing subexpression tensor shapes</h2>


<p>Most of the time, knowing the shape of the tensor variables referenced in an expression is sufficient to debug a Python statement. Sometimes, however, and operator combining the results of other operators is the source of the problem.  We need a way to visualize the shape of all partial results, which we can do with and upside down tree that shows the data flow and shape of all subexpressions. In the language world, we call that an abstract syntax tree or AST. For example, here is the fake GRU computation set up from above (with a proper <span class=inlinecode>Uxh_</span> definition):</p>


<div class="codeblk">nhidden = 256
Whh_ = torch.eye(nhidden, nhidden)
Uxh_ = torch.randn(nhidden, d)  # corrected definition
bh_  = torch.zeros(nhidden, 1)
h = torch.randn(nhidden, 1)     # fake previous hidden state h
r = torch.randn(nhidden, 1)     # fake this computation
X = torch.rand(n,d)             # fake input</div>


<p>To visualize the AST for the tensor computation, we can use <span class=inlinecode>astviz()</span> with a string representing the code to visualize, which will execute in the current execution frame:</p>


<div class="codeblk">tsensor.astviz("h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)")</div>



<a href="images/torch-gru-ast-shapes.svg">
<img nocenter="true" src="images/torch-gru-ast-shapes.svg" width="45%" url="images/torch-gru-ast-shapes.svg">
</a>



<p>If you'd like to see just the AST and don't want the statement or expression to be evaluated, pass <span class=inlinecode>frame=None</span> to <span class=inlinecode>astviz()</span>. (Side note: ASTs are generated with <span class=inlinecode>graphviz</span> but the code visualizations use <span class=inlinecode>matplotlib</span>.)</p>

<p>I'd to finish up this article by describing a little bit about how TensorSensor works.  If language implementation isn't your thing, feel free to ignore the next section, but please do check out the <a href="https://github.com/parrt/tensor-sensor">TensorSensor library</a> if you do lots of tensor computations.</p>



<h2 id="sec:1.6">1.6 Key TensorSensor implementation Kung Fu</h2>


<p>The implementation of TensorSensor leveraged my experience as a language implementor, which came in surprisingly handy in my new world as a machine learning droid.  If you have similar experience, most of the details will be clear to you in the code, but it's worth exploring the key trick to making everything you see in this article work: incrementally parsing and evaluating bits of Python statements and expressions, recording their partial results. (It's also the case that I abused <span class=inlinecode>matplotlib</span> horribly to generate the visualizations, but you can check out the code to see how that works.)</p>

<p>As I pointed out earlier, Python traps exceptions at the statement level. For example, if you get an <span class=inlinecode>IndexError</span> in <span class=inlinecode>a[i] + b[j]</span>, Python doesn't tell you  which index operation caused the exception. The same is true for tensor computations and calls to tensor libraries; only the entire statement is flagged. At first, I thought I would have to process the Python bytecode and inject code to track subexpression evaluation, but realized I could do the same thing using <span class=inlinecode>eval()</span> using substrings of the Python statement.</p>

<p>To identify the individual operator and operands that triggered an exception, we literally have to reevaluate each operation in the Python line, in the proper order, piece-by-piece and wait for an exception to occur. That, in turn, means parsing the statement to build an appropriate AST with operators as subtree roots. For example, the parser converts </p>


<div class="codeblk">h_ = torch.tanh(Whh_ @ (r*h) + Uxh_ @ X.T + bh_)</div>


<p>into this AST with all tokens as leaves:</p>

<p>
<a href="images/torch-gru-ast.svg">
<img nocenter="true" src="images/torch-gru-ast.svg" width="45%" url="images/torch-gru-ast.svg">
</a>
</p>

<p>I used the built-in Python tokenizer, but built my own parser for the subset of statements (assignments, <span class=inlinecode>return</span>) and operations supported by TensorSensor. There is a built-in Python parser, but it generates a different AST than I wanted; plus, filtering the built-in tree structure for just the parts I care about would be about the same amount of work as the parser.  I built the parser using simple recursive descent, rather than adding another dependency (my friend <span class=inlinecode>antlr</span>) to TensorSensor. (See module <span class=inlinecode>tsensor.parser</span>.)</p>

<p>Next, we need to augment the AST with the values of all subexpressions so it looks like the AST from the previous section.  This is a straightforward bottom-up walk of the AST calling <span class=inlinecode>eval()</span> on each subexpression, saving the result in the associated node. In order to execute the various bits of the expression in the proper context, though, TensorSensor has to walk the call stack back down to the specific context that directly invoked the tensor library.  The trick is not chasing the call stack to far, down into the tensor library. (See <span class=inlinecode>tsensor.analysis.deepest_frame()</span>.)</p>

<p>Once the AST is augmented with partial values, we need to find the smallest subexpressions that evaluates to a tensor for visualization purposes. That corresponds to the deepest subtree that evaluates to a tensor, which is the role of <span class=inlinecode>tsensor.analysis.smallest_matrix_subexpr()</span>.</p>

<p>Both <span class=inlinecode>clarify()</span> and <span class=inlinecode>explain()</span> use the <span class=inlinecode>with</span>-statement functions <span class=inlinecode>__enter__()</span> and <span class=inlinecode>__exit__()</span>. To initiate <span class=inlinecode>clarify()</span>, simply remembers the execution context and, while exiting, it checks for an exception (<span class=inlinecode>is_interesting_exception()</span>). If there is an exception, <span class=inlinecode>clarify()</span> passes the offending Python source line to  <span class=inlinecode>tsensor.viz.pyviz()</span>, which reevaluates the line piece-by-piece to find the operation that triggered the exception. Then it visualizes the line of code with matplotlib.</p>

<p>To visualize each line of Python code as it executes, <span class=inlinecode>explain()</span>'s <span class=inlinecode>__enter__()</span> function creates a <span class=inlinecode>ExplainTensorTracer</span> object and passes it to <span class=inlinecode>sys.settrace()</span>. Python then notifies the tracer right before each line of code is executed. The tracer parses the line and, if there's no syntax error, the tracer calls <span class=inlinecode>tsensor.viz.pyviz()</span> to visualize the statement. The <span class=inlinecode>__exit__()</span> function turns off tracing but also checks for an exception like <span class=inlinecode>clarify()</span> does.</p>

<p>Those are the key ideas, which combined with the source code, should complete the picture of TensorSensor's implementation. Feel free to contact me with questions.</p>



</body>
</html>